{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let’s now start doing transformations and working with them with our basketball dataset.\n",
    "\n",
    "##### We’ve provided you with the numerical and categorical features, it’s your turn to make a pipeline for each and then use ColumnTransformer to transform them.\n",
    "\n",
    "##### We have a regression problem this time where we are attempting to predict a player’s salary.\n",
    "\n",
    "#### Tasks:\n",
    "\n",
    "#### Import the necessary library.\n",
    "#### Create a pipeline for the numeric features. It should have the first step as simple imputation using strategy=\"median\" and the second step should be using StandardScaler. Name this pipeline numeric_transformer.\n",
    "#### Create a pipeline for the categorical features. It should also have 2 steps. The first is imputation using strategy=\"most_frequent\". The second step should be one-hot encoding with handle_unknown=\"ignore\". Name this pipeline categorical_transformer.\n",
    "#### Make your column transformer named col_transformer and specify the transformations on numeric_features and categorical_features using the appropriate pipelines you build above. -Create a main pipeline named main_pipe which preprocesses with col_transformer followed by building a KNeighborsRegressor model.\n",
    "#### The last step is performing cross-validation using our pipeline."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the necessary library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading in the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bball_df = pd.read_csv('bball_imp.csv').dropna(subset=['salary'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(bball_df, test_size=0.2, random_state=7)\n",
    "\n",
    "X_train = df_train[[\"weight\", \"height\", \"draft_year\", \"draft_round\",\n",
    "                     \"draft_peak\", \"team\", \"position\", \"country\"]]\n",
    "X_test = df_test[[\"weight\", \"height\", \"draft_year\", \"draft_round\",\n",
    "                     \"draft_peak\", \"team\", \"position\", \"country\"]]\n",
    "y_train = df_train['salary']\n",
    "y_test = df_test['salary']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split the numeric and categorical features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = [ \"weight\",\n",
    "                     \"height\",\n",
    "                     \"draft_year\",\n",
    "                     \"draft_round\",\n",
    "                     \"draft_peak\"]\n",
    "\n",
    "categorical_features = [\"team\", \"position\", \"country\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build a numeric pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a pipeline for the numeric features. It should have the first step as simple imputation using strategy=\"median\" and the second step should be using StandardScaler. Name this pipeline numeric_transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = Pipeline(\n",
    "    steps=[('imputation', SimpleImputer(strategy='median')), \n",
    "           ('scaler', StandardScaler())]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build a categorical pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a pipeline for the categorical features. It should also have 2 steps. The first is imputation using strategy=\"most_frequent\". The second step should be one-hot encoding with handle_unknown=\"ignore\". Name this pipeline categorical_transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_transformer = Pipeline(\n",
    "    steps=[('imputation', SimpleImputer(strategy='most_frequent')), \n",
    "           ('one-hot', OneHotEncoder(handle_unknown='ignore'))]\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build a column transformer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make your column transformer named col_transformer and specify the transformations on numeric_features and categorical_features using the appropriate pipelines you build above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('numeric', numeric_transformer, numeric_features),\n",
    "        ('categorical', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build a main pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a main pipeline named main_pipe which preprocesses with col_transformer followed by building a KNeighborsRegressor model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_pipe = Pipeline(\n",
    "    steps=[('transformer', col_transformer),\n",
    "        ('KNN', KNeighborsRegressor())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross validate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is performing cross-validation using our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_score</th>\n",
       "      <th>train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.011428</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.338072</td>\n",
       "      <td>0.599277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.016540</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.207216</td>\n",
       "      <td>0.646728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.017862</td>\n",
       "      <td>0.002990</td>\n",
       "      <td>0.492646</td>\n",
       "      <td>0.576790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.004983</td>\n",
       "      <td>0.002713</td>\n",
       "      <td>0.483127</td>\n",
       "      <td>0.596786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.476496</td>\n",
       "      <td>0.581861</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fit_time  score_time  test_score  train_score\n",
       "0  0.011428    0.000000    0.338072     0.599277\n",
       "1  0.016540    0.000000    0.207216     0.646728\n",
       "2  0.017862    0.002990    0.492646     0.576790\n",
       "3  0.004983    0.002713    0.483127     0.596786\n",
       "4  0.000000    0.000000    0.476496     0.581861"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_validate(main_pipe, X_train, y_train, return_train_score=True)\n",
    "pd.DataFrame(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
