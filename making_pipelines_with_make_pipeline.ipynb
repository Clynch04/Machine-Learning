{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let’s try to redo last exercise, but this time let’s use make_pipeline() and make_column_transformer.\n",
    "\n",
    "#### Tasks:\n",
    "\n",
    "##### Import the necessary library.\n",
    "##### For all pipelines, make sure to use make_pipeline() where possible.\n",
    "##### Create a pipeline for the numeric features. It should have the first step as simple imputation using strategy=\"median\" and the second step should be using StandardScaler. Name this pipeline numeric_transformer.\n",
    "##### Create a pipeline for the categorical features. It should also have 2 steps. The first is imputation using strategy=\"most_frequent\". The second step should be one-hot encoding with handle_unknown=\"ignore\". Name this pipeline categotical_transformer.\n",
    "##### Make your column transformer named col_transformer by using make_column_transformer()and specify the transformations on numeric_features and categorical_features using the appropriate pipelines you build above.\n",
    "##### Create a main pipeline named main_pipe which preprocesses with col_transformer followed by building a KNeighborsRegressor model.\n",
    "##### The last step is performing cross-validation using our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bball_df = pd.read_csv('bball_imp.csv').dropna(subset=['salary'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(bball_df, test_size=0.2, random_state=7)\n",
    "\n",
    "X_train = df_train[[\"weight\", \"height\", \"draft_year\", \"draft_round\",\n",
    "                     \"draft_peak\", \"team\", \"position\", \"country\"]]\n",
    "X_test = df_test[[\"weight\", \"height\", \"draft_year\", \"draft_round\",\n",
    "                     \"draft_peak\", \"team\", \"position\", \"country\"]]\n",
    "y_train = df_train['salary']\n",
    "y_test = df_test['salary']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split the numeric and categorical features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = [ \"weight\",\n",
    "                     \"height\",\n",
    "                     \"draft_year\",\n",
    "                     \"draft_round\",\n",
    "                     \"draft_peak\"]\n",
    "\n",
    "categorical_features = [\"team\", \"position\", \"country\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build a numeric pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a pipeline for the numeric features. It should have the first step as simple imputation using strategy=\"median\" and the second step should be using StandardScaler. Name this pipeline numeric_transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = make_pipeline(SimpleImputer(strategy='median'), StandardScaler())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build a categorical pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a pipeline for the categorical features. It should also have 2 steps. The first is imputation using strategy=\"most_frequent\". The second step should be one-hot encoding with handle_unknown=\"ignore\". Name this pipeline categotical_transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_transformer = make_pipeline(SimpleImputer(strategy='most_frequent'), OneHotEncoder(handle_unknown='ignore'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build a column transformer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make your column transformer named col_transformer by using make_column_transformer()and specify the transformations on numeric_features and categorical_features using the appropriate pipelines you build above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_transformer = make_column_transformer(\n",
    "    (numeric_transformer, numeric_features),\n",
    "    (categorical_transformer, categorical_features)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build a main pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a main pipeline named main_pipe which preprocesses with col_transformer followed by building a KNeighborsRegressor model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_pipe = make_pipeline(col_transformer, KNeighborsRegressor())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross validate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is performing cross-validation using our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_score</th>\n",
       "      <th>train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.024040</td>\n",
       "      <td>0.013961</td>\n",
       "      <td>0.338072</td>\n",
       "      <td>0.599277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.007896</td>\n",
       "      <td>0.005420</td>\n",
       "      <td>0.207216</td>\n",
       "      <td>0.646728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.006895</td>\n",
       "      <td>0.004696</td>\n",
       "      <td>0.492646</td>\n",
       "      <td>0.576790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.006430</td>\n",
       "      <td>0.005055</td>\n",
       "      <td>0.483127</td>\n",
       "      <td>0.596786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.009768</td>\n",
       "      <td>0.005053</td>\n",
       "      <td>0.476496</td>\n",
       "      <td>0.581861</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fit_time  score_time  test_score  train_score\n",
       "0  0.024040    0.013961    0.338072     0.599277\n",
       "1  0.007896    0.005420    0.207216     0.646728\n",
       "2  0.006895    0.004696    0.492646     0.576790\n",
       "3  0.006430    0.005055    0.483127     0.596786\n",
       "4  0.009768    0.005053    0.476496     0.581861"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_categorical_scores = cross_validate(main_pipe, X_train, y_train, return_train_score=True)\n",
    "pd.DataFrame(with_categorical_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
